{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c7d0eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#autograd is a package use for the gradient calculation \n",
    "#graadients are the essential key componants for out model optimization \n",
    "#the pytorch provides the autograd package that perform all the nesseay calculation for us \n",
    "#torch.autograd does the automatic differentitation by collecting all the gradients \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "46bad0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0795, 0.1180, 0.3702], requires_grad=True)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "#creating the tensor with the shape off 3 and with argumentif requires_grad=True\n",
    "x=torch.rand(3,requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c049f380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0795, 2.1180, 2.3702], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if we are performing the y=x+2 means it ceate graph \n",
    "#the graph look like this \n",
    "\"\"\"\n",
    " forward propogation\n",
    "->->->->->->->->->->->-> we calculate y \n",
    "x\n",
    " \\\n",
    "  \\\n",
    "   \\  ________\n",
    "    \\ |      |\n",
    "     \\|      |\n",
    "     /|   +  |------------->y\n",
    "    / |      |\n",
    "   /  ---------\n",
    "  /\n",
    " /\n",
    "2\n",
    "<-<-<-<-<-<-<-<-<-<-<-<- \n",
    "back propogation using the dy/dx\n",
    "\n",
    "first perform forward propogatoon later perform the back propogatiion based the loss(otginal_value-y)\n",
    "\"\"\"\n",
    "y=x+2\n",
    "y    #grad_fn=<AddBackward0> tell operation used to obtain the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "77832edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.3244, 6.4860, 7.6177], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#applying thee multiple operation on the tensor \n",
    "z=y*y+2\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "55e9b698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.8094, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=z.mean()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1af187e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3863, 1.4120, 1.5801])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to calculate the gradients  we need to class the backweord() function \n",
    "z.backward()  #perform the dz/dx\n",
    "#gradients  are stored in the x     #backward onlu performed on the single or scalar  value or 1D tensor\n",
    "#else if z is 2D tensor means this  error will return \"\"grad can be implicitly created only for scalar outputs\"\"\n",
    "\n",
    "x.grad   #tensor([6.9430 6.8489, 7.2088])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5c8e9958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before applying the method 1:x: tensor([[2.1943e+17, 7.4689e-43, 2.1942e+17, 7.4689e-43, 2.2085e+17],\n",
      "        [7.4689e-43, 2.2085e+17, 7.4689e-43, 2.2086e+17, 7.4689e-43],\n",
      "        [2.2086e+17, 7.4689e-43, 2.2090e+17, 7.4689e-43, 2.2090e+17],\n",
      "        [7.4689e-43, 2.2086e+17, 7.4689e-43, 2.2086e+17, 7.4689e-43]],\n",
      "       requires_grad=True)\n",
      "after applying the modehod 1:x: tensor([[2.1943e+17, 7.4689e-43, 2.1942e+17, 7.4689e-43, 2.2085e+17],\n",
      "        [7.4689e-43, 2.2085e+17, 7.4689e-43, 2.2086e+17, 7.4689e-43],\n",
      "        [2.2086e+17, 7.4689e-43, 2.2090e+17, 7.4689e-43, 2.2090e+17],\n",
      "        [7.4689e-43, 2.2086e+17, 7.4689e-43, 2.2086e+17, 7.4689e-43]])\n",
      "before applying the method 2:x: tensor([[9.0919e-39, 9.2755e-39, 2.9389e-39, 1.0286e-38, 1.0469e-38],\n",
      "        [1.0653e-38, 1.0194e-38, 2.9389e-39, 1.0194e-38, 9.9184e-39],\n",
      "        [2.9389e-39, 1.0194e-38, 2.9389e-39, 9.2755e-39, 9.0918e-39],\n",
      "        [1.0010e-38, 9.9184e-39, 1.0653e-38, 9.1837e-39, 9.6428e-39]],\n",
      "       requires_grad=True)\n",
      "after applying the modehod 2:x: tensor([[9.0919e-39, 9.2755e-39, 2.9389e-39, 1.0286e-38, 1.0469e-38],\n",
      "        [1.0653e-38, 1.0194e-38, 2.9389e-39, 1.0194e-38, 9.9184e-39],\n",
      "        [2.9389e-39, 1.0194e-38, 2.9389e-39, 9.2755e-39, 9.0918e-39],\n",
      "        [1.0010e-38, 9.9184e-39, 1.0653e-38, 9.1837e-39, 9.6428e-39]])\n",
      "before applying the method 3:x: tensor([[9.0919e-39, 9.2755e-39, 2.9389e-39, 1.0286e-38, 1.0469e-38],\n",
      "        [1.0653e-38, 1.0194e-38, 2.9389e-39, 1.0194e-38, 9.9184e-39],\n",
      "        [2.9389e-39, 1.0194e-38, 2.9389e-39, 9.2755e-39, 9.0918e-39],\n",
      "        [1.0010e-38, 9.9184e-39, 1.0653e-38, 9.1837e-39, 9.6428e-39]],\n",
      "       requires_grad=True)\n",
      "after applying the modehod 3:x: tensor([[9.0919e-39, 9.2755e-39, 2.9389e-39, 1.0286e-38, 1.0469e-38],\n",
      "        [1.0653e-38, 1.0194e-38, 2.9389e-39, 1.0194e-38, 9.9184e-39],\n",
      "        [2.9389e-39, 1.0194e-38, 2.9389e-39, 9.2755e-39, 9.0918e-39],\n",
      "        [1.0010e-38, 9.9184e-39, 1.0653e-38, 9.1837e-39, 9.6428e-39]],\n",
      "       requires_grad=True)\n",
      "after applying the modehod 3:y: tensor([[1.8184e-38, 1.8551e-38, 5.8778e-39, 2.0571e-38, 2.0939e-38],\n",
      "        [2.1306e-38, 2.0388e-38, 5.8778e-39, 2.0388e-38, 1.9837e-38],\n",
      "        [5.8778e-39, 2.0388e-38, 5.8778e-39, 1.8551e-38, 1.8184e-38],\n",
      "        [2.0020e-38, 1.9837e-38, 2.1306e-38, 1.8367e-38, 1.9286e-38]])\n"
     ]
    }
   ],
   "source": [
    "#stopping the pytorch from creating the gradients (preventing the gradient history)\n",
    "\"\"\"\n",
    "there are three methods \n",
    "1:variable.requires_grad=False  \n",
    "2.new variable=variable.detach() this will create a new tensor that doesnot require the gradient \n",
    "3.waping the code on the with torch.torch.no_grad(): the code inside this will not create gradient  \n",
    "\"\"\"\n",
    "x=torch.empty(4,5,requires_grad=True)\n",
    "#methoad1\n",
    "print(\"before applying the method 1:x:\",x)\n",
    "x.requires_grad=False\n",
    "print(\"after applying the modehod 1:x:\",x) #requires_grad=True dispear means if we pass this as a parameter means the optimiation will not be performed .\n",
    "\n",
    "\n",
    "\n",
    "#method2 \n",
    "x=torch.empty(4,5,requires_grad=True)\n",
    "print(\"before applying the method 2:x:\",x)\n",
    "x.detach_()  #applying the inplace operation\n",
    "print(\"after applying the modehod 2:x:\",x) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#method3 \n",
    "x=torch.empty(4,5,requires_grad=True)\n",
    "print(\"before applying the method 3:x:\",x)\n",
    "with torch.no_grad():\n",
    "    y=x*2\n",
    "print(\"after applying the modehod 3:x:\",x)\n",
    "print(\"after applying the modehod 3:y:\",y) #here y doesnot have the gradient because it is created withing the wap o the the with torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5b53237a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output gradients at iteration:0,graadients:tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3.])\n",
      "model output gradients at iteration:1,graadients:tensor([6., 6., 6., 6., 6., 6., 6., 6., 6., 6.])\n",
      "model output gradients at iteration:2,graadients:tensor([9., 9., 9., 9., 9., 9., 9., 9., 9., 9.])\n",
      "model output gradients at iteration:3,graadients:tensor([12., 12., 12., 12., 12., 12., 12., 12., 12., 12.])\n",
      "model output gradients at iteration:4,graadients:tensor([15., 15., 15., 15., 15., 15., 15., 15., 15., 15.])\n"
     ]
    }
   ],
   "source": [
    "#creating the some training excerize \n",
    "weights = torch.ones(10,requires_grad=True)\n",
    "for i in range(5):\n",
    "    model_output= (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(f\"model output gradients at iteration:{i},graadients:{weights.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e04d4e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output gradients at iteration:0,graadients:tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3.])\n",
      "model output gradients at iteration:1,graadients:tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3.])\n",
      "model output gradients at iteration:2,graadients:tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3.])\n",
      "model output gradients at iteration:3,graadients:tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3.])\n",
      "model output gradients at iteration:4,graadients:tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "#main problem in the above cell is got every epoch the gradients should be zero because the previous gradients should not effect the frature ateration \n",
    "weights = torch.ones(10,requires_grad=True)\n",
    "for i in range(5):\n",
    "    model_output= (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(f\"model output gradients at iteration:{i},graadients:{weights.grad}\")\n",
    "    #now converting the gradoents into 0 \n",
    "    #weights.grad=torch.zeros(weights.grad.size()[0])\n",
    "    #or\n",
    "    weights.grad.zero_()\n",
    "    #both do the same operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b8fa702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights before optimizer: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "weights after optimizer step: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#importing the torch optimizer from the torch.optim package \n",
    "#impoting the SGD  optimizer \n",
    "from torch.optim import SGD\n",
    "weights = torch.ones(10,requires_grad=True)\n",
    "print(\"weights before optimizer:\",weights)\n",
    "#in optimizer we need to pass the weights which are iteratable but tensor is not iteratable so we will pass tensor as list\n",
    "SGD_optimizer= SGD([weights],lr=0.01) #this will take the weights as the parameter and the teh lrarning rate \n",
    "#step function is resplnsible for updating the weights and biases of the neural network based on the gradients.\n",
    "SGD_optimizer.step()\n",
    "print(\"weights after optimizer step:\",weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6b13f3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights before iteration1:tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "weights after iteration1:None\n",
      "====================================\n",
      "weights before iteration2:tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "weights after iteration2:None\n",
      "====================================\n",
      "weights before iteration3:tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "weights after iteration3:None\n",
      "====================================\n",
      "weights before iteration4:tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "weights after iteration4:None\n",
      "====================================\n",
      "weights before iteration5:tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "weights after iteration5:None\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "#running the uotimizer for 3 iterations \n",
    "#creating the weiights trnsor with the size of 5 and enebling the gradients \n",
    "weights = torch.ones(10,requires_grad=True)\n",
    "SGD_optimizer=torch.optim.SGD([weights],lr=0.01)\n",
    "for i in range(5):\n",
    "    print(f\"weights before iteration{i+1}:{weights}\")\n",
    "    SGD_optimizer.step()\n",
    "    print(f\"weights after iteration{i+1}:{weights.grad}\")\n",
    "    print(\"====================================\")\n",
    "SGD_optimizer.zero_grad()  #set gradient of all the parameters to zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b36df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a3a82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
